{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5baef09a-ba07-4bf9-b4a1-d66514ee9c36",
   "metadata": {},
   "source": [
    "### 1)\n",
    "Min-Max scaling, also known as normalization, is a common data preprocessing technique used to rescale numerical features within a specific range. It transforms the values of a feature to a fixed range, typically between 0 and 1, based on the minimum and maximum values observed in the dataset.\n",
    "X \n",
    "scaled = X max−X min/X−X min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c58f9-6cd7-4e8c-9488-b35cc6f0a803",
   "metadata": {},
   "source": [
    "### 2)\n",
    "The Unit Vector technique, also known as normalization or feature scaling, rescales the features of a dataset to have a unit norm. Unlike Min-Max scaling, which scales the features to a specific range, the Unit Vector technique focuses on the direction or magnitude of the feature vectors.\n",
    "X \n",
    "scaled = X / ∥X∥\n",
    "X is the original feature vector\n",
    "X scaled is the scaled feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8c41f6-6a3e-4387-b9fb-019220bbe279",
   "metadata": {},
   "source": [
    "### 3)\n",
    "CA (Principal Component Analysis) is a statistical technique used for dimensionality reduction. It aims to transform a high-dimensional dataset into a lower-dimensional space while preserving the most important information or patterns present in the original data.\n",
    "\n",
    "The key idea behind PCA is to find a new set of variables, called principal components, that are linear combinations of the original variables. These principal components are orthogonal to each other and capture the maximum amount of variance in the data. The first principal component explains the largest possible variance, followed by the second principal component, and so on.\n",
    "\n",
    "The steps involved in performing PCA for dimensionality reduction are as follows:\n",
    "\n",
    "Standardize the data: If the original variables have different scales, it is important to standardize them to have zero mean and unit variance. This step ensures that variables with larger scales do not dominate the PCA results.\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance matrix of the standardized data. The covariance matrix shows the relationships between the different variables in the dataset.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each principal component. Higher eigenvalues correspond to more important principal components.\n",
    "\n",
    "Select the number of principal components: Decide on the number of principal components to retain based on the cumulative explained variance or a desired level of variance retention. Retaining a sufficient number of principal components ensures that most of the information in the original data is preserved.\n",
    "\n",
    "Project the data onto the selected principal components: Transform the original data onto the selected principal components to obtain a reduced-dimensional representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598db327-fda9-4396-a365-1a53e2071cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
